{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b9ccd0-40a6-45bf-9bd4-c5c11d5c4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, LineString, MultiPoint\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4fad58e-c402-4d48-83e7-03f132afd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'inst_geopackage.gpkg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae7986-36b7-480f-a2c3-c9ed6d74d21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c908f583-1694-4df5-871c-15e4136fc10a",
   "metadata": {},
   "source": [
    "### Create a GeoPackage file with multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de940fd-1744-44ed-8f0d-114369289800",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(MultiPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aaa7c1-b820-4f57-b82c-489ef8638381",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(LineString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2204fd-881e-4230-b650-7b92c7e3a84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e34906-5534-4c5d-b0fc-014279e2db51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017ebe3-2be6-40a1-aa49-f91ed60a5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(-100, 100, (num_points, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf85efd8-8096-476a-853b-32711a578448",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"testing_geom.gpkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "860c8cdb-76d7-44c5-bb6e-282ccd63b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one dataframe with points\n",
    "for ii in range(3):\n",
    "    num_points = 10\n",
    "    coords = np.random.uniform(-100, 100, (num_points, 2))\n",
    "\n",
    "    points = [Point(xx, yy) for xx, yy in coords]\n",
    "    gdf = gpd.GeoDataFrame(points, columns=['geometry'])\n",
    "\n",
    "    # Add fields to the GeoDataFrame\n",
    "    # QUESTION: Are these required? Will it auto-generate unique IDs if I leave that out?\n",
    "    #gdf['id'] = [1, 2]\n",
    "    gdf['name'] = ['Point {}'.format(ii) for ii in np.arange(num_points)]\n",
    "    gdf['metadata'] = ['Testing metadata for point {}'.format(ii) for ii in np.arange(num_points)]\n",
    "\n",
    "    gdf.crs = 'EPSG:3031' \n",
    "    gdf.to_file(filename, driver='GPKG', layer='points{:02d}'.format(ii))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cde68dc7-49f0-473d-8ed0-62b89ee40208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<POINT (72.597 94.213)>, 'Point 3', 'Testing metadata for point 3'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.values[gdf.index[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520e1dcd-8b8a-4b8f-86fc-cff20f145387",
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_frame = pd.DataFrame({'Institution': ['AWI', 'BAS', 'CRESIS', 'KOPRI', 'LDEO', 'STANFORD', 'UTIG']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2b92663-e6c4-44f5-ad3f-376fc99bb12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "with sqlite3.connect(filename) as conn:\n",
    "    institution_frame.to_sql(\"institutions\", conn, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dbf2a85-3b0b-4f82-8edd-cfb85cb37c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=10, step=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee17323-9244-49a0-a99d-d74ee58379e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create another with line strings\n",
    "linestrings = []\n",
    "for ii in range(3):\n",
    "    num_lines = 3\n",
    "    lines = [LineString(np.random.uniform(-100, 100, (10, 2))) for _ in np.arange(num_lines)]\n",
    "    gdf = gpd.GeoDataFrame(lines, columns=['geometry'])\n",
    "\n",
    "    # Add fields to the GeoDataFrame\n",
    "    #gdf['id'] = [1, 2]\n",
    "    gdf['name'] = ['Line {}'.format(ii) for ii in np.arange(num_lines)]\n",
    "    gdf.crs = 'EPSG:3031'  \n",
    "    gdf.to_file(filename, driver='GPKG', layer='lines{:02d}'.format(ii))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6117974f-e1cd-4f45-b380-37bdf5217dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n",
      "0 [<MULTIPOINT (-72.925 68.457, -75.172 14.543, -92.553 30.868, 19.21 71.311, 6...>]\n",
      "(10, 2)\n",
      "1 [<MULTIPOINT (45.902 99.333, 70.914 -28.628, 46.591 81.244, 70.957 62.563, -6...>]\n",
      "(10, 2)\n",
      "2 [<MULTIPOINT (-26.268 50.301, 69.452 -55.123, -90.608 77.305, -31.47 40.084, ...>]\n"
     ]
    }
   ],
   "source": [
    "# And yet another with MultiPoints.\n",
    "for ii in range(3):\n",
    "    num_points = 1\n",
    "    rand_pts = np.random.uniform(-100, 100, (10, 2))\n",
    "    print(rand_pts.shape)\n",
    "    points = [MultiPoint(rand_pts) for _ in np.arange(num_points)]\n",
    "    print(ii, points)\n",
    "    gdf = gpd.GeoDataFrame(points, columns=['geometry'])\n",
    "\n",
    "    # Add fields to the GeoDataFrame\n",
    "    #gdf['id'] = [1, 2]\n",
    "    gdf['name'] = ['MP {}'.format(ii) for ii in np.arange(num_points)]\n",
    "    gdf.crs = 'EPSG:3031'  \n",
    "    gdf.to_file(filename, driver='GPKG', layer='mp{:02d}'.format(ii))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d04bfb2-ab3f-4542-82d0-65295ce6f4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method to_file in module geopandas.geodataframe:\n",
      "\n",
      "to_file(filename, driver=None, schema=None, index=None, **kwargs) method of geopandas.geodataframe.GeoDataFrame instance\n",
      "    Write the ``GeoDataFrame`` to a file.\n",
      "    \n",
      "    By default, an ESRI shapefile is written, but any OGR data source\n",
      "    supported by Fiona can be written. A dictionary of supported OGR\n",
      "    providers is available via:\n",
      "    \n",
      "    >>> import fiona\n",
      "    >>> fiona.supported_drivers  # doctest: +SKIP\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filename : string\n",
      "        File path or file handle to write to. The path may specify a\n",
      "        GDAL VSI scheme.\n",
      "    driver : string, default None\n",
      "        The OGR format driver used to write the vector file.\n",
      "        If not specified, it attempts to infer it from the file extension.\n",
      "        If no extension is specified, it saves ESRI Shapefile to a folder.\n",
      "    schema : dict, default None\n",
      "        If specified, the schema dictionary is passed to Fiona to\n",
      "        better control how the file is written. If None, GeoPandas\n",
      "        will determine the schema based on each column's dtype.\n",
      "        Not supported for the \"pyogrio\" engine.\n",
      "    index : bool, default None\n",
      "        If True, write index into one or more columns (for MultiIndex).\n",
      "        Default None writes the index into one or more columns only if\n",
      "        the index is named, is a MultiIndex, or has a non-integer data\n",
      "        type. If False, no index is written.\n",
      "    \n",
      "        .. versionadded:: 0.7\n",
      "            Previously the index was not written.\n",
      "    mode : string, default 'w'\n",
      "        The write mode, 'w' to overwrite the existing file and 'a' to append.\n",
      "        Not all drivers support appending. The drivers that support appending\n",
      "        are listed in fiona.supported_drivers or\n",
      "        https://github.com/Toblerity/Fiona/blob/master/fiona/drvsupport.py\n",
      "    crs : pyproj.CRS, default None\n",
      "        If specified, the CRS is passed to Fiona to\n",
      "        better control how the file is written. If None, GeoPandas\n",
      "        will determine the crs based on crs df attribute.\n",
      "        The value can be anything accepted\n",
      "        by :meth:`pyproj.CRS.from_user_input() <pyproj.crs.CRS.from_user_input>`,\n",
      "        such as an authority string (eg \"EPSG:4326\") or a WKT string.\n",
      "    engine : str, \"fiona\" or \"pyogrio\"\n",
      "        The underlying library that is used to write the file. Currently, the\n",
      "        supported options are \"fiona\" and \"pyogrio\". Defaults to \"fiona\" if\n",
      "        installed, otherwise tries \"pyogrio\".\n",
      "    **kwargs :\n",
      "        Keyword args to be passed to the engine, and can be used to write\n",
      "        to multi-layer data, store data within archives (zip files), etc.\n",
      "        In case of the \"fiona\" engine, the keyword arguments are passed to\n",
      "        fiona.open`. For more information on possible keywords, type:\n",
      "        ``import fiona; help(fiona.open)``. In case of the \"pyogrio\" engine,\n",
      "        the keyword arguments are passed to `pyogrio.write_dataframe`.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The format drivers will attempt to detect the encoding of your data, but\n",
      "    may fail. In this case, the proper encoding can be specified explicitly\n",
      "    by using the encoding keyword parameter, e.g. ``encoding='utf-8'``.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    GeoSeries.to_file\n",
      "    GeoDataFrame.to_postgis : write GeoDataFrame to PostGIS database\n",
      "    GeoDataFrame.to_parquet : write GeoDataFrame to parquet\n",
      "    GeoDataFrame.to_feather : write GeoDataFrame to feather\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    >>> gdf.to_file('dataframe.shp')  # doctest: +SKIP\n",
      "    \n",
      "    >>> gdf.to_file('dataframe.gpkg', driver='GPKG', layer='name')  # doctest: +SKIP\n",
      "    \n",
      "    >>> gdf.to_file('dataframe.geojson', driver='GeoJSON')  # doctest: +SKIP\n",
      "    \n",
      "    With selected drivers you can also append to a file with `mode=\"a\"`:\n",
      "    \n",
      "    >>> gdf.to_file('dataframe.shp', mode=\"a\")  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gdf.to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e76344-4705-4d37-87bd-8777e7d4d422",
   "metadata": {},
   "source": [
    "### Trying to create one with the bedmap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d25e7b75-707a-48b4-9b76-2372d389513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_xy(filepath):\n",
    "    '''Open file in the BEDMAP CVS format and extract just lat long from it'''\n",
    "    skip_lines = count_skip_lines(filepath)\n",
    "    data = pd.read_csv(filepath, skiprows=skip_lines)\n",
    "\n",
    "    x_index = [col for col in data.columns if 'easting' in col][0]\n",
    "    y_index = [col for col in data.columns if 'northing' in col][0]\n",
    "    xx = data[x_index]\n",
    "    yy = data[y_index]\n",
    "    return np.array(xx), np.array(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b3a5e02-f71a-4b2a-8761-0e47a6525187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_skip_lines(filepath):\n",
    "    \"\"\"\n",
    "    Count how many comment lines are at the start of a CSV file.\n",
    "\n",
    "    QGS's vector layer API doesn't appear to have a setting that says\n",
    "    \"ignore all lines starting with a '#'\", even though the GUI seems\n",
    "    to auto-detect that and fill in the number.\n",
    "    \"\"\"\n",
    "    skip_lines = 0\n",
    "    for line in open(filepath, 'r'):\n",
    "        if line.startswith(\"#\"):\n",
    "            skip_lines += 1\n",
    "        else:\n",
    "            break\n",
    "    return skip_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70a59073-e0e8-47e0-9499-81815bf45291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpkg_layer(layer_name, gpkg_filepath, csv_filepaths, geometry_names, color):\n",
    "    \"\"\"\"\n",
    "    Add data in the CSV to the Geopackage, and create a QGIS layer with\n",
    "    that as the data backend.\n",
    "\n",
    "    We can't just directly drag the Geopackage into QGIS because:\n",
    "    1) I don't see how to group layers within the GeoPackage file\n",
    "    2) Programmatically styling the geopackage is awkward. I've only\n",
    "      figured out how to do it when running in the QGIS console.\n",
    "    \"\"\"\n",
    "    print(\"Called create_gpkg_layer!\")\n",
    "    print(\"layer_name = {}\".format(layer_name))\n",
    "    print(\"gpkg_filepath = {}\".format(gpkg_filepath))\n",
    "    print(\"csv_filepaths = {}\".format(csv_filepaths))\n",
    "    print(\"geometry_names = {}\".format(geometry_names))\n",
    "    print(\"color = {}\".format(color))\n",
    "    # Add a layer with these features to the GeoPackage\n",
    "    geometries = []\n",
    "    for filepath in csv_filepaths:\n",
    "        xx, yy = load_xy(filepath)\n",
    "        coords = np.array([[x1, y1] for x1, y1 in zip(xx, yy)])\n",
    "\n",
    "        print(coords.shape)\n",
    "        points = MultiPoint(coords)\n",
    "        #print(points)\n",
    "        geometries.append(points)\n",
    "        print(\"ln 266\")\n",
    "    print(geometries)\n",
    "    gdf = gpd.GeoDataFrame(geometries, columns=['geometry'])\n",
    "    print(\"ln 267\")\n",
    "    gdf['name'] = geometry_names\n",
    "    print(\"ln 269\")\n",
    "    gdf.crs = 'EPSG:3031'\n",
    "    \n",
    "    gdf.to_file(gpkg_filepath, driver='GPKG', layer=layer_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce4cd895-2a4a-459d-a557-0c3a0f05effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called create_gpkg_layer!\n",
      "layer_name = 1994_DML1\n",
      "gpkg_filepath = /Users/lindzey/RadarData/targ/qiceradar_index.gpkg\n",
      "csv_filepaths = ['/Users/lindzey/RadarData/targ/ANTARCTIC/BEDMAP/AWI/AWI_1994_DML1_AIR_BM2.csv']\n",
      "geometry_names = ['1994_DML1']\n",
      "color = 251,154,153,255\n",
      "(22578, 2)\n",
      "ln 266\n",
      "[<MULTIPOINT (-285560.755 1557735.604, -285182.186 1557356.256, -284979.372 1...>]\n",
      "ln 267\n",
      "ln 269\n"
     ]
    }
   ],
   "source": [
    "csv_filepaths = [\"/Users/lindzey/RadarData/targ/ANTARCTIC/BEDMAP/AWI/AWI_1994_DML1_AIR_BM2.csv\"]\n",
    "gpkg_filepath = \"test.gpkg\"\n",
    "geometry_names = [\"AWI_1994_DML1\"]\n",
    "layer_name = \"AWI_1994_DML1\"\n",
    "salmon = \"251,154,153,255\"\n",
    "\n",
    "layer_name = \"1994_DML1\"\n",
    "gpkg_filepath = \"/Users/lindzey/RadarData/targ/qiceradar_index.gpkg\"\n",
    "csv_filepaths = ['/Users/lindzey/RadarData/targ/ANTARCTIC/BEDMAP/AWI/AWI_1994_DML1_AIR_BM2.csv']\n",
    "geometry_names = ['1994_DML1']\n",
    "color = \"251,154,153,255\"\n",
    "\n",
    "\n",
    "create_gpkg_layer(layer_name, gpkg_filepath, csv_filepaths, geometry_names, color)\n",
    "# WTF. I copy-and-pasted failing code, and it works....?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c3b24-6b73-4c52-bc2c-e6b8f54c1409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e54658-605c-4d98-aa26-d64f190d402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gdf.to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e38ea9-123c-4141-8092-646f29bc8dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c97adf3-7408-4188-ba28-60c24db6b3ed",
   "metadata": {},
   "source": [
    "### Can I get multiple groups loaded from a geopackage file? \n",
    "\n",
    "I'd like to be able to group layers by institution ...\n",
    "\n",
    "Otherwise, I guess I could just publish one gpkg for each provider. \n",
    "\n",
    "OR, manually create the qlr file using the gpkg as the backend? If I look at a single layer, I see:\n",
    "```\n",
    "layer.source()\n",
    "\n",
    "'/Users/lindzey/Documents/QIceRadar/radar_wrangler/code/data_exploration/my_geopackage.gpkg|layername=lines00'\n",
    "```\n",
    "Suggesting that I could load it like I've been doing, using that as the URI.\n",
    "Then I'd have to figure out symbology, yet again ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c3c3f-0528-4b86-b861-494e4e0d2063",
   "metadata": {},
   "source": [
    "### What about styling?\n",
    "\n",
    "ChatGPT really struggled here, repeatedly telling me to access nonexistant methods.\n",
    "\n",
    "I got partway there in python, but then segfaulted: \n",
    "(not compatible with jupyter-lab, since my qgis install uses a different python)\n",
    "```\n",
    "from qgis.core import QgsApplication, QgsVectorLayer\n",
    "from PyQt5.QtGui import QColor\n",
    "\n",
    "qgs = QgsApplication([],False)\n",
    "\n",
    "qgs.initQgis()\n",
    "layer = QgsVectorLayer(\"my_geopackage.gpkg\", \"lines02\", \"ogr\")            \n",
    "layer.renderer().symbol().setColor(QColor(\"red\"))\n",
    "\n",
    "#layer.renderer().setSymbol(symbol)\n",
    "#layer.saveDefaultStyle()\n",
    "```\n",
    "\n",
    "It might be useful to look at {save,load}NamedStyle et al.\n",
    "saveStyleToDatabase seems likely to be the call that the GUI uses, given that the fields match exactly. \n",
    "```\n",
    "saveStyleToDatabase(...) method of qgis._core.QgsVectorLayer instance\n",
    "    saveStyleToDatabase(self, name: str, description: str, useAsDefault: bool, uiFileContent: str) -> str\n",
    "    Saves named and sld style of the layer to the style table in the db.\n",
    "    \n",
    "    :param name: Style name\n",
    "    :param description: A description of the style\n",
    "    :param useAsDefault: Set to ``True`` if style should be used as the default style for the layer\n",
    "    :param uiFileContent:\n",
    "(END)\n",
    "```\n",
    "\n",
    "Unfortunately, that dies with: \n",
    "```\n",
    ">>> layer.saveStyleToDatabase('test_red', 'testing, testing', True, '')\n",
    "libc++abi: Pure virtual function called!\n",
    "zsh: abort      python3\n",
    "```\n",
    "\n",
    "I tried using the SQLite DB Browser to create a new row in the layer_styles table, but that didn't work to set style for that layer, even though I copied it directly. \n",
    "\n",
    "I tried loading it via the dialog (rather than implicitly by drag-and-drop), and actually got a more informative error:\n",
    "\n",
    "`The retrieved style is not a valid named style. Error message: Root <qgis> element could not be found`\n",
    "\n",
    "Maybe I actually needed the 10k charactter styleQML field too, even though earlier experiments suggested styleSLD was enough?\n",
    "\n",
    "There's also `{save,load}SldStyle`...\n",
    "\n",
    "If I do it from *within* the Python Console in QGIS, this works:\n",
    "```\n",
    "layer = iface.activeLayer()\n",
    "layer.renderer().symbol().setColor(QColor(\"cyan\"))\n",
    "layer.triggerRepaint()\n",
    "layer.saveStyleToDatabase('test_cyan', '', True, '')\n",
    "```\n",
    "\n",
    "OK, so next attempt will be a script that goes through and sets the style for each layer. \n",
    "\n",
    "* Q1: Can I have a table of styles, and only specify style_name for a layer? Or do I have to have a separate row in the style_table for each layer?\n",
    "* Q2: Assuming that I have to assign styles in a separate step from the layer creation, how can I programmatically access metadata indicating what color a layer whould be?\n",
    "```\n",
    "ss = layer.source().split('|')[0]\n",
    "\n",
    "ss = QgsLineSymbol()\n",
    "ss.setColor(QColor(\"red\"))\n",
    "rr = QgsSingleSymbolRenderer(ss)\n",
    "layer.setRenderer(rr)\n",
    "\n",
    "However, neither of the \"save\" commands I can find seem to work!\n",
    "layer.saveNamedStyle(\"my_geopackage.gpkg\")\n",
    "layer.saveStyleToDatabase('test_red', '', True, '')\n",
    "```\n",
    "\n",
    "TO iterate through the layers of the file one I've dragged the GeoPackage into QGIS:\n",
    "```\n",
    "layer_tree_view = iface.layerTreeView()\n",
    "nodes = layer_tree_view.selectedNodes()\n",
    "node = nodes[0]\n",
    "for child in nodes.children():\n",
    "    ss = child.layer().renderer().symbol()\n",
    "    ss.setColor(QColor(\"cyan\"))\n",
    "    child.layer().triggerRepaint()\n",
    "```\n",
    "This gets them all cyan, though the key in the ToC doesn't update. Close the layer and QGIS will write changes to teh gpkg file. Re-drag them in, and everything is cyan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b77749-8a97-42ba-a43b-e959b59be6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6482143-081d-472e-9223-607165d6d558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2452895-562c-4048-b27b-8e31105f7833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c98f5f-4cde-4dd1-8347-a5e030284b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c174c3-2ef0-4002-913e-1f42f90bf79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0faa63da-f74c-4904-b605-d6130e4a29fb",
   "metadata": {},
   "source": [
    "### Using sqlite3 to look at underlying structure of database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a5cdb8b-74b6-4f05-a400-95bcdfa63a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = \"chatgpt_geopackage.gpkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "817a74f8-206a-4c2b-b7ae-8f2ba807218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"/Users/lindzey/RadarData/targ/qiceradar_index.gpkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03fb20e3-8c19-4e20-b0ba-0aa846ad8793",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = \"my_geopackage2.gpkg-shm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbc9dceb-3224-4cd4-af09-523375afd416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tables(filename):\n",
    "    with sqlite3.connect(filename) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT name FROM sqlite_master WHERE type=\"table\"')\n",
    "        tables = cursor.fetchall()\n",
    "        for table in tables:\n",
    "            tablename = table[0]\n",
    "            print(\"{}:\".format(tablename))\n",
    "            cursor.execute('PRAGMA table_info({})'.format(tablename))\n",
    "            #for row in cursor:\n",
    "            #    print(row)\n",
    "            fields = [row[1] for row in cursor]\n",
    "            print(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00f138bc-23e9-4922-aaad-82924d2c43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tables(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7655d-8a13-42c2-a4c0-703a325c827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tables(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a39ee-661e-40bd-bdc8-7116cf4a5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tables(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec465630-64f1-458a-9aaa-e2b3e35a78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(filename, tablename, maxrows=None):\n",
    "    with sqlite3.connect(filename) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT * FROM {}'.format(tablename))\n",
    "        print(\"{}:\".format(tablename))\n",
    "        numrows = 0\n",
    "        for row in cursor:\n",
    "            #print(row)\n",
    "            #output = [type(elem) for elem in row]\n",
    "            output = [elem if type(elem) is not bytes else \"blob\"\n",
    "                      for elem in row]\n",
    "            print(output)\n",
    "            numrows += 1\n",
    "            if maxrows is not None and numrows >= maxrows:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ff28e21-27bf-4e21-b989-24143ea91b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpkg_contents:\n",
      "['1994_DML1', 'features', '1994_DML1', '', '2023-01-03T20:46:15.958Z', -709554.6979392086, 978681.179640682, -198292.9658415384, 1714829.881174039, 3031]\n",
      "['AWI_1994_DML1', 'features', 'AWI_1994_DML1', '', '2023-01-04T16:53:38.227Z', -709554.6979392086, 978681.179640682, -198292.9658415384, 1714829.881174039, 3031]\n",
      "['AWI_1995_DML2', 'features', 'AWI_1995_DML2', '', '2023-01-04T16:53:38.691Z', -430662.1741941844, 1398761.277751747, 767915.2728331031, 2156149.372818692, 3031]\n"
     ]
    }
   ],
   "source": [
    "read_table(index, 'gpkg_contents', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c456b76a-cb67-4bc9-beb6-8c0e85ff5835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpkg_geometry_columns:\n",
      "['1994_DML1', 'geom', 'MULTIPOINT', 3031, 0, 0]\n",
      "['AWI_1994_DML1', 'geom', 'MULTIPOINT', 3031, 0, 0]\n",
      "['AWI_1995_DML2', 'geom', 'MULTIPOINT', 3031, 0, 0]\n",
      "['AWI_1996_DML3', 'geom', 'MULTIPOINT', 3031, 0, 0]\n",
      "['AWI_1997_DML4', 'geom', 'MULTIPOINT', 3031, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# I think these are the ones that I need\n",
    "read_table(index, 'gpkg_geometry_columns', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13f84e55-848c-4312-8913-3cd09d495efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWI_1994_DML1:\n",
      "[1, 'blob', 'AWI_1994_DML1', None, 'AWI', None, None, '1994_DML1', 'u']\n"
     ]
    }
   ],
   "source": [
    "read_table(index, 'AWI_1994_DML1', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16a2a143-e0b8-4102-93be-79025133d30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpkg_geometry_columns:\n",
      "['points00', 'geom', 'POINT', 3031, 0, 0]\n",
      "['points01', 'geom', 'POINT', 3031, 0, 0]\n",
      "['points02', 'geom', 'POINT', 3031, 0, 0]\n",
      "['lines00', 'geom', 'LINESTRING', 3031, 0, 0]\n",
      "['lines01', 'geom', 'LINESTRING', 3031, 0, 0]\n",
      "['lines02', 'geom', 'LINESTRING', 3031, 0, 0]\n",
      "['mp00', 'geom', 'MULTIPOINT', 3031, 0, 0]\n",
      "['mp01', 'geom', 'MULTIPOINT', 3031, 0, 0]\n",
      "['mp02', 'geom', 'MULTIPOINT', 3031, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "read_table(filename, 'gpkg_geometry_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "423c5be6-2976-48e5-9cf0-9509d71220a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points00:\n",
      "[1, 'blob', 'Point 0']\n",
      "[2, 'blob', 'Point 1']\n",
      "[3, 'blob', 'Point 2']\n",
      "[4, 'blob', 'Point 3']\n",
      "[5, 'blob', 'Point 4']\n",
      "[6, 'blob', 'Point 5']\n",
      "[7, 'blob', 'Point 6']\n",
      "[8, 'blob', 'Point 7']\n",
      "[9, 'blob', 'Point 8']\n",
      "[10, 'blob', 'Point 9']\n"
     ]
    }
   ],
   "source": [
    "read_table(filename, 'points00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9afa5149-9f36-446d-b5f1-b38adcbdfb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_columns(filename, tablename):\n",
    "    with sqlite3.connect(filename) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT * FROM {}'.format(tablename))\n",
    "        columns = [elem[0] for elem in cursor.description]\n",
    "        print(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6961a165-9c02-4f11-95eb-04338b8abb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['table_name', 'column_name', 'geometry_type_name', 'srs_id', 'z', 'm']\n"
     ]
    }
   ],
   "source": [
    "list_columns(index, 'gpkg_geometry_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78d010d8-9f04-428e-8862-736cec2fc4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fid', 'geom', 'name', 'uri', 'institution', 'granule', 'segment', 'campaign', 'availability']\n"
     ]
    }
   ],
   "source": [
    "list_columns(index, 'AWI_1994_DML1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d45007c8-1162-40c4-b2f0-4cdce4cdfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_granule_availability(filename, tablename):\n",
    "    with sqlite3.connect(filename) as conn:\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        cursor = conn.execute('SELECT * FROM {}'.format(tablename))\n",
    "        data = {row['name']: row['availability'] for row in cursor}\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cfa5f4f4-1a24-4945-a56b-d8b645d0bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AWI_1994_DML1': 'u'}\n"
     ]
    }
   ],
   "source": [
    "list_granule_availability(index, 'AWI_1994_DML1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d32447dd-5b61-4f1e-84a2-90b4265ecae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = set([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "600a7050-731c-4137-8d66-8ad4572761e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfoo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'size'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "954f57c5-f790-4d91-84a7-023cf61e6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NASA_2010_ICEBRIDGE\n",
      "Processing AWI_2002_DML8\n",
      "Processing RNRF_2009_RAEap5\n",
      "Processing NIPR_2012_JARE54\n",
      "Processing UTIG_1991_CASERTZ\n",
      "Processing RNRF_2004_Mirny-Vostok\n",
      "Processing AWI_2003_DML9\n",
      "Processing INGV_1997_Talos-Dome\n",
      "Processing RNRF_2003_AMSap5\n",
      "Processing NIPR_2018_JARE60\n",
      "Processing AWI_2018_DML-Coast\n",
      "Processing RNRF_2014_RAE\n",
      "Processing ULB_2012_ICECON\n",
      "Processing UTIG_1999_SOAR-LVS-WLK\n",
      "Processing AWI_2004_DML10\n",
      "Processing UTIG_2008_ICECAP\n",
      "Processing AWI_1998_DML5\n",
      "Processing RNRF_2015_RAE\n",
      "Processing CRESIS_2009_AntarcticaTO\n",
      "Processing STOLAF_2001_ITASE-Ellsworth\n",
      "Processing KOPRI_2017_KRT1\n",
      "Processing RNRF_2006_Komsom-Vostok\n",
      "Processing RNRF_2003_48RAEap5\n",
      "Processing UWASHINGTON_2018_South-Pole-Lake\n",
      "Processing PRIC_2018_CHA4\n",
      "Processing AWI_2013_GEA-IV\n",
      "Processing UTIG_1998_West-Marie-Byrd-Land\n",
      "Processing BAS_2007_Rutford\n",
      "Processing STOLAF_2002_ITASE-Byrd-South-Pole\n",
      "Processing UTIG_2013_GIMBLE\n",
      "Processing RNRF_2011_RAE\n",
      "Processing AWI_1994_DML1\n",
      "Processing BAS_2001_MAMOG\n",
      "Processing PRIC_2004_CHINARE-21\n",
      "Processing STOLAF_2001_ITASE-Byrd-Ellsworth\n",
      "Processing NPI_2010_SRM\n",
      "Processing BAS_2002_TORUS\n",
      "Processing NPI_2016_MADICE\n",
      "Processing BAS_2018_Thwaites\n",
      "Processing RNRF_2013_RAE\n",
      "Processing PRIC_2016_CHA2\n",
      "Processing BAS_2007_Lake-Ellsworth\n",
      "Processing RNRF_2008_53RAEap5\n",
      "Processing NASA_2009_ICEBRIDGE\n",
      "Processing UTIG_2015_EAGLE\n",
      "Processing AWI_2001_DML7\n",
      "Processing NIPR_2007_JASE\n",
      "Processing RNRF_2018_RAE\n",
      "Processing NASA_2011_ICEBRIDGE\n",
      "Processing NASA_2014_ICEBRIDGE\n",
      "Processing UCANTERBURY_2008_Darwin-Hatherton\n",
      "Processing NIPR_1992_JARE33\n",
      "Processing RNRF_2006_KV1-area\n",
      "Processing NIPR_2007_JARE49\n",
      "Processing RNRF_2005_AMSap5\n",
      "Processing RNRF_2017_RAE\n",
      "Processing RNRF_2013_Vostok-Progress\n",
      "Processing BAS_2019_Thwaites\n",
      "Processing NPI_2012_ICERISES\n",
      "Processing BGR_1999_GANOVEX-VIII-Mertz\n",
      "Processing BAS_1994_Evans\n",
      "Processing BAS_2012_Castle\n",
      "Processing RNRF_2019_RAE\n",
      "Processing RNRF_2005_50RAEap5\n",
      "Processing LDEO_2007_Recovery-Lakes\n",
      "Processing AWI_2005_ANTSYSO\n",
      "Processing RNRF_1975_Filchner-Ronne\n",
      "Processing RNRF_2007_Mirny-Vostok\n",
      "Processing BAS_2008_Lake-Ellsworth\n",
      "Processing RNRF_1971_Lambert-Amery\n",
      "Processing CRESIS_2009_Thwaites\n",
      "Processing BAS_2010_PIG\n",
      "Processing NIPR_1999_JARE40\n",
      "Processing UTIG_2009_Darwin-Hatherton\n",
      "Processing CRESIS_2013_Siple-Coast\n",
      "Processing RNRF_2010_RAE\n",
      "Processing BEDMAP1_BEDMAP1\n",
      "Processing NIPR_1996_JARE37\n",
      "Processing RNRF_1975_Lazarev\n",
      "Processing PRIC_2017_CHA3\n",
      "Processing NASA_2016_ICEBRIDGE\n",
      "Processing NPI_2015_POLARGAP\n",
      "Processing NASA_2018_ICEBRIDGE\n",
      "Processing NIPR_2017_JARE59\n",
      "Processing AWI_2018_JURAS\n",
      "Processing KOPRI_2018_KRT2\n",
      "Processing AWI_1996_DML3\n",
      "Processing RNRF_2008_Vostok-Subglacial-Lake\n",
      "Processing STOLAF_2002_ITASE-Hercules-Dome\n",
      "Processing AWI_2000_DML6\n",
      "Processing INGV_1999_Talos-Dome\n",
      "Processing UTIG_2010_ICECAP\n",
      "Processing STOLAF_1994_Siple-Dome\n",
      "Processing BAS_2007_TIGRIS\n",
      "Processing RNRF_2016_RAE\n",
      "Processing NASA_2019_ICEBRIDGE\n",
      "Processing RNRF_2004_49RAEap5\n",
      "Processing AWI_2019_JURAS\n",
      "Processing NPI_2008_BELISSIMA\n",
      "Processing BAS_1998_Dufek\n",
      "Processing AWI_1995_DML2\n",
      "Processing PRIC_2015_CHA1\n",
      "Processing BAS_2013_ISTAR\n",
      "Processing PRIC_2007_CHINARE-24\n",
      "Processing NASA_2002_ICEBRIDGE\n",
      "Processing CECS_2006_Subglacial-Lake-CECs\n",
      "Processing AWI_2018_ANIRES\n",
      "Processing AWI_2014_Recovery-Glacier\n",
      "Processing BAS_2001_Bailey-Slessor\n",
      "Processing INGV_2003_Talos-Dome\n",
      "Processing BGR_1999_GANOVEX-VIII-Matusevich\n",
      "Processing RNRF_2004_AMSap5\n",
      "Processing UTIG_2016_OLDICE\n",
      "Processing UTIG_2004_AGASEA\n",
      "Processing AWI_2007_ANTR\n",
      "Processing BGR_2002_PCMEGA\n",
      "Processing AWI_2015_GEA-DML\n",
      "Processing INGV_2001_Talos-Dome\n",
      "Processing INGV_1997_ITASE\n",
      "Processing ULB_2012_BEWISE\n",
      "Processing NASA_2013_ICEBRIDGE\n",
      "Processing BAS_2009_FERRIGNO\n",
      "Processing RNRF_2008_AMSap5\n",
      "Processing RNRF_2007_52RAEap5\n",
      "Processing STANFORD_1971_SPRI-NSF-TUD\n",
      "Processing RNRF_2006_51RAEap5\n",
      "Processing NASA_2017_ICEBRIDGE\n",
      "Processing AWI_2016_OIR\n",
      "Processing UTIG_2000_Robb-Glacier\n",
      "Processing RNRF_2006_RAEap5\n",
      "Processing AWI_1997_DML4\n",
      "Processing NASA_2004_ICEBRIDGE\n",
      "Processing RNRF_2007_AMSap5\n",
      "Processing BAS_2011_Adelaide\n",
      "Processing LDEO_2015_ROSETTA\n",
      "135 granules from 20 institutions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    institutions = set()\n",
    "    granules = set()\n",
    "    with sqlite3.connect(gpkg_filepath) as conn:\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        cursor = conn.execute('SELECT * FROM {}'.format('gpkg_geometry_columns'))\n",
    "        for row in cursor:\n",
    "            granules.add(row['table_name'])  # I think this is also the primary key\n",
    "\n",
    "        for granule in granules:\n",
    "            print(\"Processing {}\".format(granule))\n",
    "            cursor = conn.execute(\"SELECT * FROM '{}'\".format(granule))\n",
    "            # NOTE: With the current database design, these tables only have one row.\n",
    "            for row in cursor:\n",
    "                institutions.add(row['institution'])\n",
    "\n",
    "    print(\"{} granules from {} institutions\".format(len(granules), len(institutions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d17617d0-e201-44dd-b007-515c1ffe962a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AWI',\n",
       " 'BAS',\n",
       " 'BEDMAP1',\n",
       " 'BGR',\n",
       " 'CECS',\n",
       " 'CRESIS',\n",
       " 'INGV',\n",
       " 'KOPRI',\n",
       " 'LDEO',\n",
       " 'NASA',\n",
       " 'NIPR',\n",
       " 'NPI',\n",
       " 'PRIC',\n",
       " 'RNRF',\n",
       " 'STANFORD',\n",
       " 'STOLAF',\n",
       " 'UCANTERBURY',\n",
       " 'ULB',\n",
       " 'UTIG',\n",
       " 'UWASHINGTON'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "709be046-ac07-4e52-881e-d03be3215f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AWI_1994_DML1',\n",
       " 'AWI_1995_DML2',\n",
       " 'AWI_1996_DML3',\n",
       " 'AWI_1997_DML4',\n",
       " 'AWI_1998_DML5',\n",
       " 'AWI_2000_DML6',\n",
       " 'AWI_2001_DML7',\n",
       " 'AWI_2002_DML8',\n",
       " 'AWI_2003_DML9',\n",
       " 'AWI_2004_DML10',\n",
       " 'AWI_2005_ANTSYSO',\n",
       " 'AWI_2007_ANTR',\n",
       " 'AWI_2013_GEA-IV',\n",
       " 'AWI_2014_Recovery-Glacier',\n",
       " 'AWI_2015_GEA-DML',\n",
       " 'AWI_2016_OIR',\n",
       " 'AWI_2018_ANIRES',\n",
       " 'AWI_2018_DML-Coast',\n",
       " 'AWI_2018_JURAS',\n",
       " 'AWI_2019_JURAS',\n",
       " 'BAS_1994_Evans',\n",
       " 'BAS_1998_Dufek',\n",
       " 'BAS_2001_Bailey-Slessor',\n",
       " 'BAS_2001_MAMOG',\n",
       " 'BAS_2002_TORUS',\n",
       " 'BAS_2007_Lake-Ellsworth',\n",
       " 'BAS_2007_Rutford',\n",
       " 'BAS_2007_TIGRIS',\n",
       " 'BAS_2008_Lake-Ellsworth',\n",
       " 'BAS_2009_FERRIGNO',\n",
       " 'BAS_2010_PIG',\n",
       " 'BAS_2011_Adelaide',\n",
       " 'BAS_2012_Castle',\n",
       " 'BAS_2013_ISTAR',\n",
       " 'BAS_2018_Thwaites',\n",
       " 'BAS_2019_Thwaites',\n",
       " 'BEDMAP1_BEDMAP1',\n",
       " 'BGR_1999_GANOVEX-VIII-Matusevich',\n",
       " 'BGR_1999_GANOVEX-VIII-Mertz',\n",
       " 'BGR_2002_PCMEGA',\n",
       " 'CECS_2006_Subglacial-Lake-CECs',\n",
       " 'CRESIS_2009_AntarcticaTO',\n",
       " 'CRESIS_2009_Thwaites',\n",
       " 'CRESIS_2013_Siple-Coast',\n",
       " 'INGV_1997_ITASE',\n",
       " 'INGV_1997_Talos-Dome',\n",
       " 'INGV_1999_Talos-Dome',\n",
       " 'INGV_2001_Talos-Dome',\n",
       " 'INGV_2003_Talos-Dome',\n",
       " 'KOPRI_2017_KRT1',\n",
       " 'KOPRI_2018_KRT2',\n",
       " 'LDEO_2007_Recovery-Lakes',\n",
       " 'LDEO_2015_ROSETTA',\n",
       " 'NASA_2002_ICEBRIDGE',\n",
       " 'NASA_2004_ICEBRIDGE',\n",
       " 'NASA_2009_ICEBRIDGE',\n",
       " 'NASA_2010_ICEBRIDGE',\n",
       " 'NASA_2011_ICEBRIDGE',\n",
       " 'NASA_2013_ICEBRIDGE',\n",
       " 'NASA_2014_ICEBRIDGE',\n",
       " 'NASA_2016_ICEBRIDGE',\n",
       " 'NASA_2017_ICEBRIDGE',\n",
       " 'NASA_2018_ICEBRIDGE',\n",
       " 'NASA_2019_ICEBRIDGE',\n",
       " 'NIPR_1992_JARE33',\n",
       " 'NIPR_1996_JARE37',\n",
       " 'NIPR_1999_JARE40',\n",
       " 'NIPR_2007_JARE49',\n",
       " 'NIPR_2007_JASE',\n",
       " 'NIPR_2012_JARE54',\n",
       " 'NIPR_2017_JARE59',\n",
       " 'NIPR_2018_JARE60',\n",
       " 'NPI_2008_BELISSIMA',\n",
       " 'NPI_2010_SRM',\n",
       " 'NPI_2012_ICERISES',\n",
       " 'NPI_2015_POLARGAP',\n",
       " 'NPI_2016_MADICE',\n",
       " 'PRIC_2004_CHINARE-21',\n",
       " 'PRIC_2007_CHINARE-24',\n",
       " 'PRIC_2015_CHA1',\n",
       " 'PRIC_2016_CHA2',\n",
       " 'PRIC_2017_CHA3',\n",
       " 'PRIC_2018_CHA4',\n",
       " 'RNRF_1971_Lambert-Amery',\n",
       " 'RNRF_1975_Filchner-Ronne',\n",
       " 'RNRF_1975_Lazarev',\n",
       " 'RNRF_2003_48RAEap5',\n",
       " 'RNRF_2003_AMSap5',\n",
       " 'RNRF_2004_49RAEap5',\n",
       " 'RNRF_2004_AMSap5',\n",
       " 'RNRF_2004_Mirny-Vostok',\n",
       " 'RNRF_2005_50RAEap5',\n",
       " 'RNRF_2005_AMSap5',\n",
       " 'RNRF_2006_51RAEap5',\n",
       " 'RNRF_2006_KV1-area',\n",
       " 'RNRF_2006_Komsom-Vostok',\n",
       " 'RNRF_2006_RAEap5',\n",
       " 'RNRF_2007_52RAEap5',\n",
       " 'RNRF_2007_AMSap5',\n",
       " 'RNRF_2007_Mirny-Vostok',\n",
       " 'RNRF_2008_53RAEap5',\n",
       " 'RNRF_2008_AMSap5',\n",
       " 'RNRF_2008_Vostok-Subglacial-Lake',\n",
       " 'RNRF_2009_RAEap5',\n",
       " 'RNRF_2010_RAE',\n",
       " 'RNRF_2011_RAE',\n",
       " 'RNRF_2013_RAE',\n",
       " 'RNRF_2013_Vostok-Progress',\n",
       " 'RNRF_2014_RAE',\n",
       " 'RNRF_2015_RAE',\n",
       " 'RNRF_2016_RAE',\n",
       " 'RNRF_2017_RAE',\n",
       " 'RNRF_2018_RAE',\n",
       " 'RNRF_2019_RAE',\n",
       " 'STANFORD_1971_SPRI-NSF-TUD',\n",
       " 'STOLAF_1994_Siple-Dome',\n",
       " 'STOLAF_2001_ITASE-Byrd-Ellsworth',\n",
       " 'STOLAF_2001_ITASE-Ellsworth',\n",
       " 'STOLAF_2002_ITASE-Byrd-South-Pole',\n",
       " 'STOLAF_2002_ITASE-Hercules-Dome',\n",
       " 'UCANTERBURY_2008_Darwin-Hatherton',\n",
       " 'ULB_2012_BEWISE',\n",
       " 'ULB_2012_ICECON',\n",
       " 'UTIG_1991_CASERTZ',\n",
       " 'UTIG_1998_West-Marie-Byrd-Land',\n",
       " 'UTIG_1999_SOAR-LVS-WLK',\n",
       " 'UTIG_2000_Robb-Glacier',\n",
       " 'UTIG_2004_AGASEA',\n",
       " 'UTIG_2008_ICECAP',\n",
       " 'UTIG_2009_Darwin-Hatherton',\n",
       " 'UTIG_2010_ICECAP',\n",
       " 'UTIG_2013_GIMBLE',\n",
       " 'UTIG_2015_EAGLE',\n",
       " 'UTIG_2016_OLDICE',\n",
       " 'UWASHINGTON_2018_South-Pole-Lake'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "granules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71d7eb-7af9-468e-9b89-44f352bf03a2",
   "metadata": {},
   "source": [
    "## Figure out geometry type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4600737e-768f-402f-aac5-1882408efe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpkg_filepath = \"/Users/lindzey/RadarData/targ/qiceradar_index.gpkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d6e6f593-630c-4a0c-a09c-4ab9c56cc071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campaign STANFORD_SPRI_NSF_TUD has geometry LINESTRING\n"
     ]
    }
   ],
   "source": [
    "campaign = \"STANFORD_SPRI_NSF_TUD\"\n",
    "#campaign = \"UTIG_2004_AGASEA\"\n",
    "with sqlite3.connect(gpkg_filepath) as conn:\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    cursor = conn.execute(\"SELECT * FROM {}\".format('gpkg_geometry_columns'))\n",
    "    for row in cursor:\n",
    "        if row['table_name'] == campaign:\n",
    "            print(\"Campaign {} has geometry {}\".format(campaign, row['geometry_type_name']))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740e4f1-bce7-4924-8a58-f15210524edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
