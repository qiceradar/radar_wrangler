{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bd6ac2-0ead-4aef-8be1-b0cfc5912729",
   "metadata": {},
   "source": [
    "The landing page for this project seems to be: \n",
    "    \n",
    "https://pgg.ldeo.columbia.edu/data/agap-gambit\n",
    "\n",
    "The READMEs linked there appear to refer to a much older data release -- they direct me to ftp://gravity.ldeo.columbia.edu, where the agap/agap login doesn't seem to work anymore.\n",
    "\n",
    "Instead, the data is at: http://wonder.ldeo.columbia.edu/data/AGAP/DataLevel_1/RADAR/index.html\n",
    "I assume we'll want the SAR data: http://wonder.ldeo.columbia.edu/data/AGAP/DataLevel_1/RADAR/DecimatedSAR_netcdf/index.html\n",
    "\n",
    "The directory structure is {F,L,T,V}####/F##_L##-###_1D_SAR.nc, and each netCDF file is ~16M.\n",
    "The two L (lines) that I checked are about 2G each.\n",
    "\n",
    "Link for first chunk of L270: http://wonder.ldeo.columbia.edu/data/AGAP/DataLevel_1/RADAR/DecimatedSAR_netcdf/L270/F31b_L270-181_1D_SAR.nc\n",
    "\n",
    "The READMEs attached here only mention matlab files, and talk about known issues with the preliminary data release."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3093161a-e328-481e-8e19-5eeab051fdb5",
   "metadata": {},
   "source": [
    "Line-based naming convention:\n",
    "\n",
    "    * L -- long\n",
    "    * T -- tie\n",
    "    * F -- connecting to Dome Fujii survey\n",
    "    * V -- connecting to Lake Vostok survey\n",
    "    \n",
    "And, old matlab files split into 3.5km along-track chunks. Ouch!! netCDF does the same. Uggggh! This is testing my resolve to directly work with whatever formats the providers supply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d837976a-125a-4295-83e7-68b977c873f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # For downloading index page\n",
    "from bs4 import BeautifulSoup   # For parsing html and extracting the links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040b23e5-012a-4649-a756-2636c4165682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by iterating over all links in the index\n",
    "ldeo_agap_sar = \"http://wonder.ldeo.columbia.edu/data/AGAP/DataLevel_1/RADAR/DecimatedSAR_netcdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df91c00d-a1b9-41d9-b765-da2b67153368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F10130/index.html', 'F10150/index.html', 'F10170/index.html', 'F10190/index.html', 'F10210/index.html', 'F10230/index.html', 'L270/index.html', 'L280/index.html', 'L290/index.html', 'L300/index.html', 'L310/index.html', 'L320/index.html', 'L330/index.html', 'L340/index.html', 'L350/index.html', 'L360/index.html', 'L370/index.html', 'L380/index.html', 'L390/index.html', 'L400/index.html', 'L410/index.html', 'L420/index.html', 'L430/index.html', 'L440/index.html', 'L450/index.html', 'L460/index.html', 'L470/index.html', 'L480/index.html', 'L490/index.html', 'L500/index.html', 'L510/index.html', 'L520/index.html', 'L530/index.html', 'L540/index.html', 'L550/index.html', 'L560/index.html', 'L570/index.html', 'L580/index.html', 'L590/index.html', 'L600/index.html', 'L610/index.html', 'L620/index.html', 'L630/index.html', 'L640/index.html', 'L650/index.html', 'L660/index.html', 'L670/index.html', 'L680/index.html', 'L690/index.html', 'L700/index.html', 'L710/index.html', 'L720/index.html', 'L730/index.html', 'L740/index.html', 'L750/index.html', 'L760/index.html', 'T10090/index.html', 'T10100/index.html', 'T10110/index.html', 'T10120/index.html', 'T10130/index.html', 'T10140/index.html', 'T10150/index.html', 'T10160/index.html', 'T10170/index.html', 'T10180/index.html', 'T10190/index.html', 'T10200/index.html', 'T10210/index.html', 'T10220/index.html', 'T10230/index.html', 'T10240/index.html', 'V10130/index.html', 'V10150/index.html', 'V10170/index.html', 'V10190/index.html', 'V10210/index.html', 'V10230/index.html']\n"
     ]
    }
   ],
   "source": [
    "reqs = requests.get(ldeo_agap_sar + \"/index.html\")\n",
    "soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "line_urls = [link.get('href') for link in soup.find_all('a')]\n",
    "print(line_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0307d7-219b-48a7-98a2-ab7777956e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Their documentation uses \"flight line\" to refer to what UTIG calls transects, \n",
    "# and then \"file number\" for the segment IDs that they're split up into.\n",
    "file_count = 0\n",
    "for line_url in line_urls:\n",
    "    line = line_url.split('/')[0]\n",
    "    #print(\"Handling: {}\".format(line))\n",
    "    \n",
    "    reqs = requests.get('/'.join([ldeo_agap_sar, line_url]))\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "    file_urls = [link.get('href') for link in soup.find_all('a')]\n",
    "    #print('\\n'.join(file_urls))\n",
    "    #print(\"...{} files\".format(len(file_urls)))\n",
    "    file_count += len(file_urls)\n",
    "    \n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b810237f-0023-4e1a-a0d8-8eedc01672af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.45195312500002"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How big do we expect the downlaod to be? Spot-checking gave 15.8 M for a single file\n",
    "file_count * 15.8 / 1024  # Convert to GB.  OK, 150G isn't that bad to download all of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da3d3f3-6c4a-4ef5-b782-aa2b831d6cb2",
   "metadata": {},
   "source": [
    "### Given that I just want every file in that directory structure, is there something better than iterating manually?\n",
    "~~~\n",
    "QICERADAR_DATA=/Volumes/RadarData\n",
    "LDEO_DIR=${QICERADAR_DATA}/LDEO/AGAP_GAMBIT\n",
    "mkdir -p $LDEO_DIR\n",
    "cd $LDEO_DIR\n",
    "\n",
    "wget -c -r -d -nH --cut-dirs=5 http://wonder.ldeo.columbia.edu/data/AGAP/DataLevel_1/RADAR/DecimatedSAR_netcdf/index.html\n",
    "~~~\n",
    "* -r for recursive\n",
    "* -d for debug\n",
    "* -nH --cut-dirs=5 should disable the deep directory structure\n",
    "  * -nH removes the URL from the start\n",
    "  * --cut-dirs removes data/AGAP/DataLevel_1/RADAR/DecimatedSAR_netcdf \n",
    "* -c for continue (checks number of bytes, tries to append to end of file rather than overwriting) (I haven't tested this yet)\n",
    "\n",
    "This can just be run repeatedly. (And will need to be, as I'm getting ~500 kB/sec - 2.5 MB/sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61c960-c3a4-4a14-9bc9-545ed0e51393",
   "metadata": {},
   "source": [
    "#### What about the source listed in the Bedmap compilation? DOI: http://get.iedadata.org/doi/317765\n",
    "\n",
    "It claims to use the \"Marine Geoscience Data System\", but the link is to USAP-DC\n",
    "\n",
    "And, as usual, USAP-DC says:\n",
    "\"Due to its size only a list of the file names is directly available, the actual data are available on request from info@usap-dc.org\"\n",
    "\n",
    "So, I think I should tell people to cite the DOI, but automate the download from Columbia's website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16535b-bdc2-4bf7-a98c-40d96b43c1ed",
   "metadata": {},
   "source": [
    "# Rosetta\n",
    "\n",
    "Appears to be available as a single 200G download (!!)\n",
    "\n",
    "\n",
    "* example code for Matlab: http://wonder.ldeo.columbia.edu/data/ROSETTA-Ice/Radar/RS_Process_Example/DICE-master.zip\n",
    "* 200G download for DICE (deep ice radar): http://wonder.ldeo.columbia.edu/data/ROSETTA-Ice/Radar/RS_Process_Example/Rosetta_Data.zip\n",
    "\n",
    "When I e-mailed Kirsty a year ago, she said that they were trying to move it to \"formal access\", and gave me chunked example data for a single line:\n",
    "    https://drive.google.com/drive/folders/10MvRe21Jj7xZepDdCauBt7gIC-hybul2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc54e9f-82e9-4b1e-a0a0-fc50821d0f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02c241-cac4-4177-937d-580a1e38090b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753325e-77c9-42ce-8a36-039f69f9b7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97cdeb-27f0-4fae-b99c-dc60af1258b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6f025e4-c548-48b2-8d10-1d2ec8947e2b",
   "metadata": {},
   "source": [
    "# Greenland!\n",
    "\n",
    "Looks like they have some 2014 Greenland data.\n",
    "\n",
    "https://pgg.ldeo.columbia.edu/data/icepod\n",
    "\n",
    "I haven't yet figured out how I plan to segment Arctic/Antarctic data; probably just separate top-level directories in my data drive for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f94fe0-b7d4-4cc9-bb9d-bda04c38fa3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
